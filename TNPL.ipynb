{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Replace with the actual URL you want to scrape\n",
    "#url = \"https://www.espncricinfo.com/series/tnpl-2023-1379595/match-schedule-fixtures-and-results\"\n",
    "#url= \"https://www.espncricinfo.com/series/tnpl-2022-1312873/match-schedule-fixtures-and-results\"\n",
    "#url = \"https://www.espncricinfo.com/series/tnpl-2021-1261607/match-schedule-fixtures-and-results\"\n",
    "url = \"https://www.espncricinfo.com/series/tnpl-2024-1439628/match-schedule-fixtures-and-results\"\n",
    "\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url, verify = False)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all <a> tags (links)\n",
    "link_tags = soup.find_all('a')\n",
    "\n",
    "filtered_id_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Extract numeric values from the href attributes\n",
    "for link in link_tags:\n",
    "    href = link.get('href')\n",
    "    numeric_values = re.findall(r'\\d+', href)  # Extract all numeric values\n",
    "    if numeric_values:\n",
    "        print(f\"Link: {href}, Numeric Values: {', '.join(numeric_values)}\")\n",
    "    else:\n",
    "        print(f\"No numeric values found in link: {href}\")\n",
    "\n",
    "                # Filter out all 7-digit numeric values\n",
    "    filtered_ids = [match for match in numeric_values if match.isdigit() and match != '1439628']\n",
    "\n",
    "# Print the filtered IDs\n",
    "    for link_id in filtered_ids:\n",
    "        if link_id.isdigit() and link_id != '1439628':\n",
    "            filtered_id_list.append(link_id)\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the integer IDs from the href attributes\n",
    "# for link in link_tags:\n",
    "#     href = link.get('href')\n",
    "#     match = re.search(r'/(\\d{7})', href)  # Adjust the regex pattern as needed\n",
    "#     if match:\n",
    "#         link_id = match.group(1)\n",
    "#         print(f\"Link: {href}, ID: {link_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1439761, 1439762, 1439763, 1439764, 1439765, 1439766, 1439767, 1439768, 1439769, 1439770, 1439771, 1439772, 1439773, 1439774, 1439775, 1439776, 1439777, 1439778, 1439779, 1439780, 1439781, 1439782, 1439783, 1439784, 1439785, 1439786, 1439787, 1439788, 1439789, 1439790, 1439791, 1439792]\n"
     ]
    }
   ],
   "source": [
    "# Filter for 7-digit integers starting with '1422'\n",
    "\n",
    "\n",
    "seven_digit_integers = [int(num) for num in filtered_id_list if num.isdigit() and \n",
    "                        1_000_000 <= int(num) < 10_000_000 and num.startswith('1439')]\n",
    "#seven_digit_integers\n",
    "def get_unique_list(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item not in unique_list:\n",
    "            unique_list.append(item)\n",
    "    return unique_list\n",
    "\n",
    "unique_values = get_unique_list(seven_digit_integers)\n",
    "unique_values.sort()\n",
    "\n",
    "# Find the indices of the start and end values\n",
    "start_index = unique_values.index(1439761)\n",
    "# Slice the list from the index of 1415701 to the end\n",
    "unique_values_1 = unique_values[start_index:]\n",
    "\n",
    "# Print the sorted list starting from 1415701\n",
    "print(unique_values_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getMatchData(seriesId, matchId):\n",
    "    url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/overs/details\"\n",
    "    params = {\n",
    "        'lang': 'en',\n",
    "        'seriesId': seriesId,\n",
    "        'matchId': matchId,\n",
    "        'mode': 'ALL',\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'User-Agent': 'Your User Agent',  # Set an appropriate user agent\n",
    "\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers, verify=False, allow_redirects=True)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'hs-consumer-api.espncricinfo.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "statInfo    0\n",
       "short       0\n",
       "long        0\n",
       "Name: wagonZone, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = getMatchData('1445031', '1446371')\n",
    "first_innings = full_data['inningOvers'][0]['stats']\n",
    "second_innings = full_data['inningOvers'][1]\n",
    "##[1]['stats']\n",
    "\n",
    "\n",
    "#print(second_innings)\n",
    "\n",
    "# example win probability on 11th over\n",
    "balls = first_innings[1]['balls']\n",
    "# #print(balls)\n",
    "# balls = second_innings[4]\n",
    "df_test =  pd.DataFrame(balls[5])\n",
    "df_test['wagonZone']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "rows = []\n",
    "\n",
    "# Iterate over each ID and call the function for indices 1 to 20\n",
    "for id in unique_values_1:\n",
    "    # Replace the second ID in the original code with the current ID\n",
    "    full_data = getMatchData('1439628', id)\n",
    "    # Check if the data for the current ID is available\n",
    "    # Check if the data for the current ID is available\n",
    "    if full_data:\n",
    "        \n",
    "        # Check if predictions exist for the first innings\n",
    "        if 'inningOvers' in full_data and full_data['inningOvers'] and full_data['inningOvers'] is not None:\n",
    "            try:\n",
    "                predictions = full_data['inningOvers'][0]['stats']\n",
    "                \n",
    "                # Extract the predictions for indices 1 to 20\n",
    "                for i in range(len(predictions)):\n",
    "                    \n",
    "                    # Use a try-except block to catch IndexError\n",
    "                    try:\n",
    "                        #if 'predictions' in predictions[i] and predictions[i]['predictions'] is not None:\n",
    "                        if predictions[i]['balls'] is not None:\n",
    "                            \n",
    "                            balls = predictions[i]['balls']\n",
    "                            \n",
    "                            for j in range(len(balls)):\n",
    "                                # Use a nested try-except block to catch IndexError\n",
    "                                try:\n",
    "                                    bowler_stat = balls[j].get('bowlerStatText', 'N/A')\n",
    "                                    batsmanStatText = balls[j].get('batsmanStatText', 'N/A')\n",
    "                                    pitchline1 = balls[j].get('pitchLine', 'N/A')\n",
    "                                    pitchlength1 = balls[j].get('pitchLength', 'N/A')\n",
    "                                    dismissalType = balls[j].get('dismissalType', 'N/A')\n",
    "                                    bowlerPlayerId = balls[j].get('bowlerPlayerId', 'N/A')\n",
    "                                    batsmanPlayerId = balls[j].get('batsmanPlayerId', 'N/A'),\n",
    "                                    batsmanRuns = balls[j].get('batsmanRuns', 'N/A'),\n",
    "                                    wagonX = balls[j].get('wagonX', 'N/A'),\n",
    "                                    wagonY = balls[j].get('wagonY', 'N/A'),\n",
    "                                    wagonZone = balls[j].get('wagonZone', 'N/A')\n",
    "                                    shotType = balls[j].get('shotType', 'N/A')\n",
    "                                    shotControl = balls[j].get('shotControl', 'N/A')\n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "                                    row = {\n",
    "                                        'id': id,\n",
    "                                        'over': i,\n",
    "                                        'ball': j,\n",
    "                                        'bowler_stat': bowler_stat,\n",
    "                                        'batsmanStatText': batsmanStatText,\n",
    "                                        'pitchline1': pitchline1,\n",
    "                                        'pitchlength1': pitchlength1,\n",
    "                                        'dismissalType': dismissalType,\n",
    "                                        'bowlerPlayerId': bowlerPlayerId,\n",
    "                                        'batsmanPlayerId': batsmanPlayerId,\n",
    "                                        'batsmanRuns': batsmanRuns,\n",
    "                                        'wagonY': wagonY,\n",
    "                                        'wagonX': wagonX,\n",
    "                                        'wagonZone': wagonZone,\n",
    "                                        'shotType': shotType,\n",
    "                                        'shotControl': shotControl\n",
    "                                    }\n",
    "                                    rows.append(row)\n",
    "                                except IndexError:\n",
    "                                    # Skip if there's no data for the current ball\n",
    "                                    continue\n",
    "                    except IndexError:\n",
    "                        # Skip if there's no data for the current prediction\n",
    "                        print(f\"No predictions available for index {i} of ID {id}.\")\n",
    "                        continue\n",
    "            except IndexError:\n",
    "                # Skip if there's no data for the second element of 'inningOvers'\n",
    "                print(f\"No inning data available for ID {id}.\")\n",
    "        else:\n",
    "            print(f\"Skipping ID {id} as no data is available.\")\n",
    "\n",
    "\n",
    "       # Store the predictions in the results dictionary\n",
    "\n",
    "tnpl_1stinnings_2024 = pd.DataFrame(rows)       \n",
    "#tnpl_2ndinnings = pd.DataFrame(rows)                 \n",
    "# Now you can access the predictions for each ID and index\n",
    "#for key, second_innings in results_by_id.items():\n",
    "#    print(f\"Predictions for {key}: {second_innings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "rows = []\n",
    "\n",
    "# Iterate over each ID and call the function for indices 1 to 20\n",
    "for id in unique_values_1:\n",
    "    # Replace the second ID in the original code with the current ID\n",
    "    full_data = getMatchData('1439628', id)\n",
    "    # Check if the data for the current ID is available\n",
    "    # Check if the data for the current ID is available\n",
    "    if full_data:\n",
    "        \n",
    "        # Check if predictions exist for the first innings\n",
    "        if 'inningOvers' in full_data and full_data['inningOvers'] and full_data['inningOvers'] is not None:\n",
    "            try:\n",
    "                predictions = full_data['inningOvers'][1]['stats']\n",
    "                \n",
    "                # Extract the predictions for indices 1 to 20\n",
    "                for i in range(len(predictions)):\n",
    "                    \n",
    "                    # Use a try-except block to catch IndexError\n",
    "                    try:\n",
    "                        #if 'predictions' in predictions[i] and predictions[i]['predictions'] is not None:\n",
    "                        if predictions[i]['balls'] is not None:\n",
    "                            \n",
    "                            balls = predictions[i]['balls']\n",
    "                            \n",
    "                            for j in range(len(balls)):\n",
    "                                # Use a nested try-except block to catch IndexError\n",
    "                                try:\n",
    "                                    bowler_stat = balls[j].get('bowlerStatText', 'N/A')\n",
    "                                    batsmanStatText = balls[j].get('batsmanStatText', 'N/A')\n",
    "                                    pitchline1 = balls[j].get('pitchLine', 'N/A')\n",
    "                                    pitchlength1 = balls[j].get('pitchLength', 'N/A')\n",
    "                                    dismissalType = balls[j].get('dismissalType', 'N/A')\n",
    "                                    bowlerPlayerId = balls[j].get('bowlerPlayerId', 'N/A')\n",
    "                                    batsmanPlayerId = balls[j].get('batsmanPlayerId', 'N/A'),\n",
    "                                    batsmanRuns = balls[j].get('batsmanRuns', 'N/A'),\n",
    "                                    wagonX = balls[j].get('wagonX', 'N/A'),\n",
    "                                    wagonY = balls[j].get('wagonY', 'N/A'),\n",
    "                                    wagonZone = balls[j].get('wagonZone', 'N/A')\n",
    "                                    shotType = balls[j].get('shotType', 'N/A')\n",
    "                                    shotControl = balls[j].get('shotControl', 'N/A')\n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "                                    row = {\n",
    "                                        'id': id,\n",
    "                                        'over': i,\n",
    "                                        'ball': j,\n",
    "                                        'bowler_stat': bowler_stat,\n",
    "                                        'batsmanStatText': batsmanStatText,\n",
    "                                        'pitchline1': pitchline1,\n",
    "                                        'pitchlength1': pitchlength1,\n",
    "                                        'dismissalType': dismissalType,\n",
    "                                        'bowlerPlayerId': bowlerPlayerId,\n",
    "                                        'batsmanPlayerId': batsmanPlayerId,\n",
    "                                        'batsmanRuns': batsmanRuns,\n",
    "                                        'wagonY': wagonY,\n",
    "                                        'wagonX': wagonX,\n",
    "                                        'wagonZone': wagonZone,\n",
    "                                        'shotType': shotType,\n",
    "                                        'shotControl': shotControl\n",
    "                                    }\n",
    "                                    rows.append(row)\n",
    "                                except IndexError:\n",
    "                                    # Skip if there's no data for the current ball\n",
    "                                    continue\n",
    "                    except IndexError:\n",
    "                        # Skip if there's no data for the current prediction\n",
    "                        print(f\"No predictions available for index {i} of ID {id}.\")\n",
    "                        continue\n",
    "            except IndexError:\n",
    "                # Skip if there's no data for the second element of 'inningOvers'\n",
    "                print(f\"No inning data available for ID {id}.\")\n",
    "        else:\n",
    "            print(f\"Skipping ID {id} as no data is available.\")\n",
    "\n",
    "\n",
    "       # Store the predictions in the results dictionary\n",
    "\n",
    "#tnpl_1stinnings_2024 = pd.DataFrame(rows)       \n",
    "tnpl_2ndinnings_2024 = pd.DataFrame(rows)                 \n",
    "# Now you can access the predictions for each ID and index\n",
    "#for key, second_innings in results_by_id.items():\n",
    "#    print(f\"Predictions for {key}: {second_innings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_21 = pd.concat([tnpl_1stinnings_2024, tnpl_2ndinnings_2024], ignore_index=True)\n",
    "combined_df_21['batsmanPlayerId'] = combined_df_21['batsmanPlayerId'].apply(lambda x: x[0])\n",
    "combined_df_21['batsmanRuns'] = combined_df_21['batsmanRuns'].apply(lambda x: x[0])\n",
    "combined_df_21['wagonY'] = combined_df_21['wagonY'].apply(lambda x: x[0])\n",
    "combined_df_21['wagonX'] = combined_df_21['wagonX'].apply(lambda x: x[0])# First, merge DF1 and DF2 on 'Name' and 'Batting Style'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url_list = []\n",
    "# Function to extract all URLs containing a specific pattern\n",
    "def extract_urls(url):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url, verify=False)  # 'verify=False' is used to bypass SSL verification\n",
    "    #print(response.text)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all <a> tags (links)\n",
    "        link_tags = soup.find_all('a')\n",
    "        #print(link_tags)\n",
    "\n",
    "        filtered_id_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Extract numeric values from the href attributes\n",
    "        \n",
    "        for link in link_tags:\n",
    "            href = link.get('href')\n",
    "            filtered_id_list.append(href)\n",
    "           \n",
    "        \n",
    "        return filtered_id_list\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# URL of the website to scrape\n",
    "#website_url = 'https://www.espncricinfo.com/series/tnpl-2023-1379595/squads'\n",
    "#website_url = 'https://www.espncricinfo.com/series/tnpl-2022-1312873/squads'\n",
    "#website_url = 'https://www.espncricinfo.com/series/tnpl-2022-1261607/squads'\n",
    "website_url = 'https://www.espncricinfo.com/series/tnpl-2024-1439628/squads'\n",
    "\n",
    "\n",
    "# Extract and print the URLs\n",
    "urls = extract_urls(website_url)\n",
    "#print(urls)\n",
    "\n",
    "# Regular expression pattern to match 7 digit numbers\n",
    "pattern = re.compile(r'\\d{7}')\n",
    "\n",
    "\n",
    "# List to store the extracted numbers\n",
    "extracted_numbers = []\n",
    "\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    #print(url)\n",
    "    if url.startswith(\"/series/tnpl-2024-1439628/\"):\n",
    "        print(url)\n",
    "        # Find all 7-digit numbers in the URL\n",
    "        numbers = re.findall(r'\\d{7}', url)\n",
    "        print(numbers)\n",
    "        \n",
    "        # Add the numbers to the list if they are not '1439628'\n",
    "        for number in numbers:\n",
    "            if number != '1439628':\n",
    "                extracted_numbers.append(number)\n",
    "\n",
    "print(extracted_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1439792, 1439792, 1440002, 1440004, 1440006, 1440013, 1442069, 1440010, 1440017, 1440020, 1440021]\n",
      "[1440002, 1440004, 1440006, 1440013, 1442069, 1440010, 1440017, 1440020, 1440021]\n"
     ]
    }
   ],
   "source": [
    "list_with_integers = [int(item) for item in extracted_numbers]\n",
    "print(list_with_integers)\n",
    "# Find the index of '1440002' in the list\n",
    "index = list_with_integers.index(1440002)\n",
    "\n",
    "# Remove everything before '1440002' in the list\n",
    "new_list = list_with_integers[index:]\n",
    "\n",
    "# Print the new list\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_players_df = []\n",
    "\n",
    "# Define the list of team IDs\n",
    "#team_ids_list = [1320716,1320677,1320702,1320703,1320706,1320707,1320720,1320717]  # Replace with your actual team IDs\n",
    "team_ids_list = new_list   \n",
    "# Initialize an empty DataFrame\n",
    "\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'User-Agent': 'Your User Agent',  # Set an appropriate user agent\n",
    "}\n",
    "\n",
    "# Define the base parameters\n",
    "base_params = {\n",
    "    'lang': 'en',\n",
    "    'seriesId': 1439628,\n",
    "    \n",
    "}\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://hs-consumer-api.espncricinfo.com/v1/pages/series/squad/details'\n",
    "\n",
    "# Loop through each team ID\n",
    "for team_id in team_ids_list:\n",
    "    \n",
    "    \n",
    "    # Update the parameters with the current page and team ID\n",
    "    params = base_params.copy()\n",
    "    params.update({\n",
    "        #'page': page,\n",
    "        'squadId': team_id\n",
    "    })\n",
    "\n",
    "    # Send the request\n",
    "    response = requests.get(url, verify=False, headers=headers, params=params)\n",
    "    response_content = response.content\n",
    "    \n",
    "    # Decode the bytes to string and load it as JSON\n",
    "    response_json = json.loads(response_content.decode('utf-8'))\n",
    "    # Extract the players information\n",
    "    players_info = response_json['content']['squadDetails']['players']    \n",
    "    #print(players_info)\n",
    "    # Loop through each player\n",
    "    for player_info in players_info:\n",
    "        # Extract the player dictionary\n",
    "        player_dict = player_info['player']\n",
    "\n",
    "        \n",
    "        \n",
    "        # Create a row for the player\n",
    "        row = {\n",
    "            'ID': player_dict['id'],\n",
    "            \n",
    "            'Name': player_dict['longName'],\n",
    "            'Date of Birth': f\"{player_dict['dateOfBirth']['year']}-{player_dict['dateOfBirth']['month']}-{player_dict['dateOfBirth']['date']}\" if player_dict.get('dateOfBirth') else 'N/A',\n",
    "            'Batting Style': ', '.join(player_dict.get('battingStyles', ['N/A'])),\n",
    "            'Long Bowling Style': ', '.join(player_dict.get('longBowlingStyles', ['N/A']))\n",
    "        }\n",
    "        \n",
    "        # Append the row to the list\n",
    "        all_players_df.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "all_players_df1 = pd.DataFrame(all_players_df)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(all_players_df1)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_players_df1.to_csv('C:/Users/kripa.krishnan/OneDrive - Precision For Medicine/Desktop/TNPL_info_2024.csv', index=False)\n",
    "\n",
    "all_players_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_21 = pd.merge(combined_df_21, all_players_df1[['ID','Name', 'Batting Style']], left_on='batsmanPlayerId', right_on='ID', how='left', suffixes=('_DF2', '_DF3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Then, merge the resulting dataframe with DF2 again, but this time on 'Name' and 'Bowling Style'\n",
    "combined_df_21 = pd.merge(combined_df_21, all_players_df1[['ID','Name', 'Long Bowling Style']], left_on='bowlerPlayerId', right_on='ID', how='left', )\n",
    "\n",
    "\n",
    "#combined_df_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "matchids = unique_values_1\n",
    "\n",
    "\n",
    "headers = {\n",
    "        'accept': 'application/json',\n",
    "        'User-Agent': 'Your User Agent',  # Set an appropriate user agent\n",
    "\n",
    "    }\n",
    "\n",
    "#url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/scorecard\"\n",
    "#url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/home\"\n",
    "#url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/overs/details\"\n",
    "url = 'https://hs-consumer-api.espncricinfo.com/v1/pages/series/schedule'\n",
    "\n",
    "# Create an empty DataFrame\n",
    "data = []\n",
    "data1 = []\n",
    "\n",
    "for matchid in matchids:\n",
    "    params = {\n",
    "        'lang': 'en',\n",
    "        'seriesId': 1439628,\n",
    "         #'matchId': matchid,\n",
    "         #'mode': 'ALL',\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers, verify=False, allow_redirects=True)\n",
    "    scorecard = response.json()\n",
    "    #print(scorecard)    \n",
    "\n",
    "   # Get the slug\n",
    "    \n",
    "    slug = scorecard['content']['matches']\n",
    "    for match in slug:\n",
    "        game = match['slug']\n",
    "        id = match[\"objectId\"]\n",
    "        statusText = match['statusText']\n",
    "        \n",
    "\n",
    "    # Append the match ID and slug to the DataFrame\n",
    "        data.append({'id': id, 'slug': game, 'statusText': statusText})\n",
    "\n",
    "df_slug = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slug.to_csv('C:/Users/kripa.krishnan/OneDrive - Precision For Medicine/Desktop/TNPL_info_2024_slug.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, merge the final_df dataframe with DF3 on 'SLUGNAME'\n",
    "final_df = combined_df_21.merge(df_slug, on='id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_df.to_csv('C:/Users/kripa.krishnan/OneDrive - Precision For Medicine/Desktop/tnpl_2024_0805.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
