{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Replace with the actual URL you want to scrape\n",
    "url = \"https://www.espncricinfo.com/series/icc-men-s-t20-world-cup-2024-1411166/match-schedule-fixtures-and-results\"\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url, verify = False)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all <a> tags (links)\n",
    "link_tags = soup.find_all('a')\n",
    "\n",
    "filtered_id_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Extract numeric values from the href attributes\n",
    "for link in link_tags:\n",
    "    href = link.get('href')\n",
    "    numeric_values = re.findall(r'\\d+', href)  # Extract all numeric values\n",
    "    if numeric_values:\n",
    "        print(f\"Link: {href}, Numeric Values: {', '.join(numeric_values)}\")\n",
    "    else:\n",
    "        print(f\"No numeric values found in link: {href}\")\n",
    "\n",
    "                # Filter out all 7-digit numeric values\n",
    "    filtered_ids = [match for match in numeric_values if match.isdigit() and match != '1411166']\n",
    "\n",
    "# Print the filtered IDs\n",
    "    for link_id in filtered_ids:\n",
    "        if link_id.isdigit() and link_id != '1411166':\n",
    "            filtered_id_list.append(link_id)\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the integer IDs from the href attributes\n",
    "# for link in link_tags:\n",
    "#     href = link.get('href')\n",
    "#     match = re.search(r'/(\\d{7})', href)  # Adjust the regex pattern as needed\n",
    "#     if match:\n",
    "#         link_id = match.group(1)\n",
    "#         print(f\"Link: {href}, ID: {link_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1415702, 1415703, 1415704, 1415705, 1415706, 1415707, 1415708, 1415709, 1415710, 1415711, 1415712, 1415713, 1415714, 1415715, 1415716, 1415717, 1415718, 1415719, 1415720, 1415721, 1415722, 1415723, 1415724, 1415725, 1415726, 1415727, 1415728, 1415729, 1415730, 1415731, 1415732, 1415733, 1415734, 1415735, 1415736, 1415737, 1415738, 1415739, 1415740, 1415741, 1415742, 1415743, 1415744, 1415745, 1415746, 1415747, 1415748, 1415749, 1415750, 1415751, 1415752, 1415753, 1415754, 1415755]\n"
     ]
    }
   ],
   "source": [
    "# Filter for 7-digit integers starting with '1422'\n",
    "\n",
    "\n",
    "seven_digit_integers = [int(num) for num in filtered_id_list if num.isdigit() and 1_000_000 <= int(num) < 10_000_000 and num.startswith('141')]\n",
    "#seven_digit_integers\n",
    "def get_unique_list(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item not in unique_list:\n",
    "            unique_list.append(item)\n",
    "    return unique_list\n",
    "\n",
    "unique_values = get_unique_list(seven_digit_integers)\n",
    "unique_values.sort()\n",
    "\n",
    "# Find the indices of the start and end values\n",
    "start_index = unique_values.index(1415702)\n",
    "# Slice the list from the index of 1415701 to the end\n",
    "unique_values_1 = unique_values[start_index:]\n",
    "\n",
    "# Print the sorted list starting from 1415701\n",
    "print(unique_values_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getMatchData(seriesId, matchId):\n",
    "    url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/overs/details\"\n",
    "    params = {\n",
    "        'lang': 'en',\n",
    "        'seriesId': seriesId,\n",
    "        'matchId': matchId,\n",
    "        'mode': 'ALL',\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'User-Agent': 'Your User Agent',  # Set an appropriate user agent\n",
    "\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers, verify=False, allow_redirects=True)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'hs-consumer-api.espncricinfo.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statInfo': '10* (11b)',\n",
       " 'short': 'JN Frylinck 10* (11b 2x4)',\n",
       " 'long': 'Jan Frylinck 10* (11b 2x4)'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = getMatchData('1411166', '1415703')\n",
    "first_innings = full_data['inningOvers'][0]['stats']\n",
    "second_innings = full_data['inningOvers'][1]['stats']\n",
    "\n",
    "# example win probability on 11th over\n",
    "len(second_innings)\n",
    "balls = second_innings[4]['balls']\n",
    "balls[1]['batsmanStatText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "matchids = unique_values_1\n",
    "\n",
    "\n",
    "headers = {\n",
    "        'accept': 'application/json',\n",
    "        'User-Agent': 'Your User Agent',  # Set an appropriate user agent\n",
    "\n",
    "    }\n",
    "\n",
    "#url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/scorecard\"\n",
    "url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/home\"\n",
    "#url = \"https://hs-consumer-api.espncricinfo.com/v1/pages/match/overs/details\"\n",
    "\n",
    "# Create an empty DataFrame\n",
    "data = []\n",
    "\n",
    "for matchid in matchids:\n",
    "    params = {\n",
    "        'lang': 'en',\n",
    "        'seriesId': 1411166,\n",
    "         'matchId': matchid,\n",
    "         #'mode': 'ALL',\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers, verify=False, allow_redirects=True)\n",
    "    scorecard = response.json()\n",
    "    #print(scorecard)    \n",
    "\n",
    "   # Get the slug\n",
    "    slug = scorecard['match']['slug']\n",
    "    print(slug)\n",
    "\n",
    "    # Append the match ID and slug to the DataFrame\n",
    "    data.append({'match_id': matchid, 'slug': slug})\n",
    "\n",
    "df_slug = pd.DataFrame(data)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slug.to_csv('C:/Users/kripa.krishnan/OneDrive - Precision For Medicine/Desktop/T20_Games.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "rows = []\n",
    "\n",
    "# Iterate over each ID and call the function for indices 1 to 20\n",
    "for id in unique_values_1:\n",
    "    # Replace the second ID in the original code with the current ID\n",
    "    full_data = getMatchData('1411166', id)\n",
    "    # Check if the data for the current ID is available\n",
    "    # Check if the data for the current ID is available\n",
    "    if full_data:\n",
    "        \n",
    "        # Check if predictions exist for the first innings\n",
    "        if 'inningOvers' in full_data and full_data['inningOvers'] and full_data['inningOvers'] is not None:\n",
    "            try:\n",
    "                predictions = full_data['inningOvers'][0]['stats']\n",
    "                # Extract the predictions for indices 1 to 20\n",
    "                for i in range(len(predictions)):\n",
    "                    # Use a try-except block to catch IndexError\n",
    "                    try:\n",
    "                        if 'predictions' in predictions[i] and predictions[i]['predictions'] is not None:\n",
    "                            balls = predictions[i]['balls']\n",
    "                            for j in range(len(balls)):\n",
    "                                # Use a nested try-except block to catch IndexError\n",
    "                                try:\n",
    "                                    bowler_stat = balls[j].get('bowlerStatText', 'N/A')\n",
    "                                    batsmanStatText = balls[j].get('batsmanStatText', 'N/A')\n",
    "                                    pitchline1 = balls[j].get('pitchLine', 'N/A')\n",
    "                                    pitchlength1 = balls[j].get('pitchLength', 'N/A')\n",
    "                                    dismissalType = balls[j].get('dismissalType', 'N/A')\n",
    "\n",
    "                                    row = {\n",
    "                                        'id': id,\n",
    "                                        'over': i,\n",
    "                                        'ball': j,\n",
    "                                        'bowler_stat': bowler_stat,\n",
    "                                        'batsmanStatText': batsmanStatText,\n",
    "                                        'pitchline1': pitchline1,\n",
    "                                        'pitchlength1': pitchlength1,\n",
    "                                        'dismissalType': dismissalType\n",
    "                                    }\n",
    "                                    rows.append(row)\n",
    "                                except IndexError:\n",
    "                                    # Skip if there's no data for the current ball\n",
    "                                    continue\n",
    "                    except IndexError:\n",
    "                        # Skip if there's no data for the current prediction\n",
    "                        print(f\"No predictions available for index {i} of ID {id}.\")\n",
    "                        continue\n",
    "            except IndexError:\n",
    "                # Skip if there's no data for the second element of 'inningOvers'\n",
    "                print(f\"No inning data available for ID {id}.\")\n",
    "        else:\n",
    "            print(f\"Skipping ID {id} as no data is available.\")\n",
    "\n",
    "\n",
    "       # Store the predictions in the results dictionary\n",
    "\n",
    "#final_df_1stinnings = pd.DataFrame(rows)      \n",
    "final_df_2ndinnings = pd.DataFrame(rows)                 \n",
    "# Now you can access the predictions for each ID and index\n",
    "#for key, second_innings in results_by_id.items():\n",
    "#    print(f\"Predictions for {key}: {second_innings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_2ndinnings.to_csv('C:/Users/kripa.krishnan/OneDrive - Precision For Medicine/Desktop/dWC_T20_2ndinnings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
